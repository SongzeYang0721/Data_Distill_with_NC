{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d4f77be",
   "metadata": {},
   "source": [
    "# Demonstration of Data Distill by Neural Collapse algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dc66db",
   "metadata": {},
   "source": [
    "This notebook implements data distillation with neural collapse. The main papers considered here are https://github.com/SsnL/dataset-distillation and https://github.com/tding1/Neural-Collapse. The neural network is firstly trained to its terminal phase and then distills synthesized data as introduced. Below we configure our path to the dataset distillation packages from https://github.com/SsnL/dataset-distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cedd2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/songzeyang/Documents/GitHub/dataset-distillation/utils')\n",
    "sys.path.append('/Users/songzeyang/Documents/GitHub/Neural-Collapse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba023951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/songzeyang/Documents/GitHub/Data_Distill_with_NC',\n",
       " '/Users/songzeyang/anaconda3/lib/python311.zip',\n",
       " '/Users/songzeyang/anaconda3/lib/python3.11',\n",
       " '/Users/songzeyang/anaconda3/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/songzeyang/anaconda3/lib/python3.11/site-packages',\n",
       " '/Users/songzeyang/anaconda3/lib/python3.11/site-packages/aeosa',\n",
       " '/Users/songzeyang/Documents/GitHub/dataset-distillation/utils',\n",
       " '/Users/songzeyang/Documents/GitHub/Neural-Collapse',\n",
       " '/Users/songzeyang/Documents/GitHub/dataset-distillation/utils',\n",
       " '/Users/songzeyang/Documents/GitHub/Neural-Collapse',\n",
       " '/Users/songzeyang/Documents/GitHub/dataset-distillation/utils',\n",
       " '/Users/songzeyang/Documents/GitHub/Neural-Collapse']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8269e011",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73a216a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import copy\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from six import add_metaclass\n",
    "from typing import Type, Any, Callable, Union, List, Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecee61",
   "metadata": {},
   "source": [
    "Let's import the file from the https://github.com/tding1/Neural-Collapse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa96b4c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'make_dataset' from 'datasets' (/Users/songzeyang/anaconda3/lib/python3.11/site-packages/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_dataset\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'make_dataset' from 'datasets' (/Users/songzeyang/anaconda3/lib/python3.11/site-packages/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "import models\n",
    "from utils import *\n",
    "from datasets import make_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254799b",
   "metadata": {},
   "source": [
    "Let's import the file from the https://github.com/SsnL/dataset-distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "705173fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from basics import task_loss, final_objective_loss, evaluate_steps\n",
    "# from utils.distributed import broadcast_coalesced, all_reduce_coalesced\n",
    "# from utils.io import save_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a726a274",
   "metadata": {},
   "source": [
    "# Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8948ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# architecture params\n",
    "model='resnet18'\n",
    "\n",
    "# dataset\n",
    "dataset='mnist'\n",
    "data_dir='~/data'\n",
    "\n",
    "# training params\n",
    "optimizer=\"LBFGS\"\n",
    "lr=0.1\n",
    "history_size=10\n",
    "batch_size=128\n",
    "uid=\"result\"\n",
    "device = \"cpu\"\n",
    "SOTA=False\n",
    "\n",
    "# Network params\n",
    "loss = 'CrossEntropy'\n",
    "bias=True\n",
    "ETF_fc=False\n",
    "fixdim=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22270111",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb808ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, _, num_classes = make_dataset(dataset, \n",
    "                                           data_dir, \n",
    "                                           batch_size, \n",
    "                                           SOTA=SOTA)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e56ddf",
   "metadata": {},
   "source": [
    "# Load Model\n",
    "\n",
    "As we are primarily focusing on the neural collapse, we will build a large Resnet18 model to show the nerual collapse. Let's built a wrap class to facilitate the distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31913b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.__dict__[model](num_classes=num_classes, \n",
    "                                    fc_bias=bias, \n",
    "                                    ETF_fc=ETF_fc, \n",
    "                                    fixdim=fixdim, \n",
    "                                    SOTA=SOTA).to(device)\n",
    "\n",
    "print('# of model parameters: ' + str(count_network_parameters(model)))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1843cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedResNet(ResNet):  # Assuming your model is ResNet\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(ExtendedResNet, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Reture the flat version of parameter as in the data distillation paper, \n",
    "        given a model subclass nn.module\n",
    "        \"\"\"\n",
    "        w_modules_names = []\n",
    "    \n",
    "        for m in self.modules():\n",
    "            for n, p in m.named_parameters(recurse=False):\n",
    "                if p is not None:\n",
    "                    w_modules_names.append((m, n))\n",
    "            for n, b in m.named_buffers(recurse=False):\n",
    "                if b is not None:\n",
    "                    print(\"The buffer will be treated as a constant and assumed not to change during gradient steps.\")\n",
    "\n",
    "        self.weights_module_names = tuple(w_modules_names)\n",
    "        \n",
    "        # Put to correct device before we do stuff on parameters\n",
    "#         self = self.to(self.device)\n",
    "\n",
    "        ws = tuple(m._parameters[n].detach() for m, n in w_modules_names)\n",
    "        \n",
    "        print(len(set(w.dtype for w in ws)))\n",
    "\n",
    "        assert len(set(w.dtype for w in ws)) == 1\n",
    "\n",
    "        # reparam to a single flat parameter\n",
    "        self.weights_numels = tuple(w.numel() for w in ws)\n",
    "        self.weights_shapes = tuple(w.shape for w in ws)\n",
    "        with torch.no_grad():\n",
    "            flat_w = torch.cat([w.reshape(-1) for w in ws], 0)\n",
    "\n",
    "        # remove old parameters, assign the names as buffers\n",
    "        for m, n in self.weights_module_names:\n",
    "            delattr(m, n)\n",
    "            m.register_buffer(n, None)\n",
    "\n",
    "        # register the flat one\n",
    "        if not hasattr(self, 'attribute_name'):\n",
    "            self.register_parameter('flat_w', nn.Parameter(flat_w, requires_grad=True))\n",
    "\n",
    "        return self.flat_w\n",
    "    \n",
    "    @contextmanager\n",
    "    def unflatten_weight(self, flat_w):\n",
    "        ws = (t.view(s) for (t, s) in zip(flat_w.split(self.weights_numels), self.weights_shapes))\n",
    "        for (m, n), w in zip(self.weights_module_names, ws):\n",
    "            setattr(m, n, w)\n",
    "        yield\n",
    "        for m, n in self.weights_module_names:\n",
    "            setattr(m, n, None)\n",
    "            \n",
    "    def forward_with_param(self, inp, new_w):\n",
    "        with self.unflatten_weight(new_w):\n",
    "#             return self.model(self, inp)\n",
    "            return nn.Module.__call__(self, inp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72036917",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtendedResNet(block=BasicBlock, layers=[2, 2, 2, 2], \n",
    "                       num_classes=num_classes, \n",
    "                       fc_bias=bias,\n",
    "                       ETF_fc=ETF_fc,\n",
    "                       fixdim=fixdim,\n",
    "                       SOTA=SOTA)\n",
    "print('# of model parameters: ' + str(count_network_parameters(model)))\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e4e201",
   "metadata": {},
   "source": [
    "However, this large model is very hard to train and we can use a smaller model to test our code base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c55d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    supported_dims = {28, 32}\n",
    "\n",
    "    def __init__(self, nc, num_classes, input_size):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nc, 6, 5, padding=2 if input_size == 28 else 0)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 1 if num_classes <= 2 else num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x), inplace=True)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out), inplace=True)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = F.relu(self.fc1(out), inplace=True)\n",
    "        out = F.relu(self.fc2(out), inplace=True)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7838cfe",
   "metadata": {},
   "source": [
    "# Data Distillation Algorithm with neural collapse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e727a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distillation settings\n",
    "distill_steps=10\n",
    "distill_epochs=3\n",
    "distilled_images_per_class_per_step=1\n",
    "distill_lr=0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6e47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(trainloader))\n",
    "size_train, channels, height, width = images.shape\n",
    "num_classes = torch.unique(labels)\n",
    "print(num_classes)\n",
    "print(labels.shape)\n",
    "print(size_train, channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a69eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distiller:\n",
    "    def __init__(self, model, device = \"cpu\", epochs = 400, history_size = 10, weight_decay = 5e-4, lr = 0.1,\n",
    "                 data = trainloader, size_train = size_train, channels = channels, height = height, width = width,\n",
    "                 distill_steps=10, \n",
    "                 distill_epochs=3, \n",
    "                 distilled_images_per_class_per_step=1,\n",
    "                 distill_lr=0.02,\n",
    "                 decay_factor = 0.5\n",
    "                ):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.history_size = history_size\n",
    "        self.weight_decay = weight_decay\n",
    "        self.lr = lr\n",
    "        # unpack the dataset\n",
    "        self.train_loader = trainloader\n",
    "        self.size_train = size_train\n",
    "        self.channels = channels\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        self.num_classes = len(torch.unique(labels))\n",
    "        # distill setting\n",
    "        self.num_data_steps = distill_steps  # how much data we have\n",
    "        self.distill_epochs = distill_epochs\n",
    "        self.distilled_images_per_class_per_step = distilled_images_per_class_per_step\n",
    "        self.distill_lr = distill_lr\n",
    "        self.decay_factor = decay_factor\n",
    "        self.T = distill_steps * distill_epochs  # how many sc steps we run\n",
    "        self.num_per_step = self.num_classes * distilled_images_per_class_per_step\n",
    "        assert distill_lr >= 0, 'distill_lr must >= 0'\n",
    "        self.init_data_optim()\n",
    "#         self.model.unflatten_weight = unflatten_weight.__get__(self.model)\n",
    "        \n",
    "    def init_data_optim(self, lr=lr):\n",
    "        self.params = []\n",
    "        optim_lr = lr\n",
    "\n",
    "        # labels\n",
    "        self.labels = []\n",
    "        distill_label = torch.arange(self.num_classes, dtype=torch.long, device=self.device) \\\n",
    "                             .repeat(self.distilled_images_per_class_per_step, 1)  # [[0, 1, 2, ...], [0, 1, 2, ...]]\n",
    "        distill_label = distill_label.t().reshape(-1)  # [0, 0, ..., 1, 1, ...]\n",
    "        for _ in range(self.num_data_steps):\n",
    "            self.labels.append(distill_label)\n",
    "        self.all_labels = torch.cat(self.labels)\n",
    "\n",
    "        # data\n",
    "        self.data = []\n",
    "        for _ in range(self.num_data_steps):\n",
    "            distill_data = torch.randn(self.num_per_step, self.channels, self.height, self.width,\n",
    "                                       device=self.device, requires_grad=True)\n",
    "            self.data.append(distill_data)\n",
    "            self.params.append(distill_data)\n",
    "        # lr\n",
    "\n",
    "        # undo the softplus + threshold\n",
    "        raw_init_distill_lr = torch.tensor(self.distill_lr, device=self.device)\n",
    "        raw_init_distill_lr = raw_init_distill_lr.repeat(self.T, 1)\n",
    "        self.raw_distill_lrs = raw_init_distill_lr.expm1_().log_().requires_grad_()\n",
    "        self.params.append(self.raw_distill_lrs)\n",
    "\n",
    "        assert len(self.params) > 0, \"must have at least 1 parameter\"\n",
    "        \n",
    "        optimizer_function = optim.LBFGS\n",
    "        kwargs = {'lr': self.lr,\n",
    "                  'history_size': self.history_size,\n",
    "                  'line_search_fn': 'strong_wolfe'\n",
    "        }\n",
    "\n",
    "        self.optimizer = optimizer_function(self.params, **kwargs)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=self.distill_epochs,\n",
    "                                                   gamma=self.decay_factor)\n",
    "        for p in self.params:\n",
    "            p.grad = torch.zeros_like(p)\n",
    "            \n",
    "    def weight_decays(self, model):\n",
    "        penalty = 0\n",
    "        for p in model.parameters():\n",
    "            if p.requires_grad:\n",
    "                penalty += 0.5 * self.weight_decay * torch.norm(p) ** 2\n",
    "        return penalty.to(self.device)\n",
    "            \n",
    "    def get_steps(self):\n",
    "        data_label_iterable = (x for _ in range(self.distill_epochs) for x in zip(self.data, self.labels))\n",
    "        lrs = F.softplus(self.raw_distill_lrs).unbind()\n",
    "\n",
    "        steps = []\n",
    "        for (data, label), lr in zip(data_label_iterable, lrs):\n",
    "            steps.append((data, label, lr))\n",
    "\n",
    "        return steps\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, model, rdata, rlabel, steps):\n",
    "\n",
    "        # forward\n",
    "        model.train()\n",
    "        w = model.get_params()\n",
    "        params = [w]\n",
    "        gws = []\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(\"loss function is made\")\n",
    "        \n",
    "        # inner loop\n",
    "        for step_i, (data, label, lr) in enumerate(steps):\n",
    "            \"\"\"\n",
    "            Begining of the inner loop. Input: distill data, output: distill labels\n",
    "            goal: store the weight for future updating.\n",
    "            \"\"\"\n",
    "            print(\"Begining of the inner loop number: \", step_i+1)\n",
    "            \n",
    "            data, label = data.to(self.device), label.to(self.device)\n",
    "            \n",
    "            with torch.enable_grad():\n",
    "                output = model.forward_with_param(data, w)\n",
    "                loss = criterion(output[0], label) + self.weight_decays(model)\n",
    "#             gw,_ = torch.autograd.grad(loss, w, lr.squeeze(), create_graph=True)\n",
    "            gw, = torch.autograd.grad(loss, w, lr.squeeze(), create_graph=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                new_w = w.sub(gw).requires_grad_() # minus the gw weighted by lr.squeeze()\n",
    "                params.append(new_w)\n",
    "                gws.append(gw)\n",
    "                w = new_w\n",
    "        \n",
    "        # final L\n",
    "        model.eval()\n",
    "        output = model.forward_with_param(rdata, params[-1])\n",
    "        ll = criterion(output[0], rlabel) + self.weight_decays(model)\n",
    "        print(\"The loss now is \", ll, \"in the forward mode\")\n",
    "        return ll, (ll, params, gws)\n",
    "    \n",
    "    def backward(self, model, rdata, rlabel, steps, saved_for_backward):\n",
    "        l, params, gws = saved_for_backward\n",
    "\n",
    "        datas = []\n",
    "        gdatas = []\n",
    "        lrs = []\n",
    "        glrs = []\n",
    "\n",
    "        dw, = torch.autograd.grad(l, (params[-1],))\n",
    "\n",
    "        # backward\n",
    "        model.train()\n",
    "       \n",
    "        for (data, label, lr), w, gw in reversed(list(zip(steps, params, gws))):\n",
    "            \n",
    "            hvp_in = [w]\n",
    "            hvp_in.append(data)\n",
    "            hvp_in.append(lr)\n",
    "            dgw = dw.neg()  # gw is already weighted by lr, so simple negation\n",
    "            hvp_grad = torch.autograd.grad(\n",
    "                outputs=(gw,),\n",
    "                inputs=hvp_in,\n",
    "                grad_outputs=(dgw,)\n",
    "            )\n",
    "            # Update for next iteration, i.e., previous step\n",
    "            with torch.no_grad():\n",
    "                # Save the computed gdata and glrs\n",
    "                datas.append(data)\n",
    "                gdatas.append(hvp_grad[1])\n",
    "                lrs.append(lr)\n",
    "                glrs.append(hvp_grad[2])\n",
    "\n",
    "                # Update for next iteration, i.e., previous step\n",
    "                # Update dw\n",
    "                # dw becomes the gradients w.r.t. the updated w for previous step\n",
    "                dw.add_(hvp_grad[0])\n",
    "\n",
    "        return datas, gdatas, lrs, glrs\n",
    "\n",
    "    def accumulate_grad(self, grad_infos):\n",
    "        bwd_out = []\n",
    "        bwd_grad = []\n",
    "        for datas, gdatas, lrs, glrs in grad_infos:\n",
    "            bwd_out += list(lrs)\n",
    "            bwd_grad += list(glrs)\n",
    "            for d, g in zip(datas, gdatas):\n",
    "                if d.grad is None:\n",
    "                    d.grad = g.clone()\n",
    "                else:\n",
    "                    d.grad.add_(g)\n",
    "        if len(bwd_out) > 0:\n",
    "            torch.autograd.backward(bwd_out, bwd_grad)\n",
    "            \n",
    "    def prefetch_train_loader_iter(self):\n",
    "\n",
    "        device = self.device\n",
    "        train_iter = iter(self.train_loader)\n",
    "        for epoch in range(self.epochs):\n",
    "            niter = len(train_iter)\n",
    "            prefetch_it = max(0, niter - 2)\n",
    "            for it, val in enumerate(train_iter):\n",
    "                # Prefetch (start workers) at the end of epoch BEFORE yielding\n",
    "                if it == prefetch_it and epoch < self.epochs - 1:\n",
    "                    train_iter = iter(self.train_loader)\n",
    "                yield epoch, it, val\n",
    "                \n",
    "    def train(self):\n",
    "\n",
    "        counts = 0\n",
    "\n",
    "        for epoch, it, (rdata, rlabel) in self.prefetch_train_loader_iter():\n",
    "\n",
    "            if it == 0:\n",
    "                self.scheduler.step()\n",
    "\n",
    "            if it == 0 and epoch == 0:\n",
    "                with torch.no_grad():\n",
    "                    steps = self.get_steps()\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            rdata, rlabel = rdata.to(self.device, non_blocking=True), rlabel.to(self.device, non_blocking=True)\n",
    "\n",
    "            losses = []\n",
    "            steps = self.get_steps()\n",
    "\n",
    "            # activate everything needed to run on this process\n",
    "            grad_infos = []\n",
    "\n",
    "            l, saved = self.forward(model, rdata, rlabel, steps)\n",
    "            losses.append(l.detach())\n",
    "            grad_infos.append(self.backward(model, rdata, rlabel, steps, saved))\n",
    "            del l, saved\n",
    "            bwd_out = []\n",
    "            bwd_grad = []\n",
    "            for datas, gdatas, lrs, glrs in grad_infos:\n",
    "                bwd_out += list(lrs)\n",
    "                bwd_grad += list(glrs)\n",
    "                for d, g in zip(datas, gdatas):\n",
    "                    if d.grad is None:\n",
    "                        d.grad = g.clone()\n",
    "                    else:\n",
    "                        d.grad.add_(g)\n",
    "            if len(bwd_out) > 0:\n",
    "                torch.autograd.backward(bwd_out, bwd_grad)\n",
    "\n",
    "            # all reduce if needed\n",
    "            # average grad\n",
    "            all_reduce_tensors = [p.grad for p in self.params]\n",
    "   \n",
    "            # opt step\n",
    "            self.optimizer.step()\n",
    "\n",
    "            del steps, grad_infos, losses, all_reduce_tensors\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            steps = self.get_steps()\n",
    "        self.save_results(steps)\n",
    "        return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6d4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "Distiller(model).train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
